#-------------------- 
# sample_net.py 
# Author: Erik Serrano 
# Email: erik.serrano@cuanschutz.edu
# Location: University of Colorado, Denver Anschutz Medical Campus
# Class: CPBS 7711
# Description:
# sample_net.py is a simple script that samples randomly generated sub networks from the input 
# and compares it to sub networks that has been generated by the STRING database. It attempts 
# to find relationships between known data and disease type data. 
#-------------------- 


# standard lib imports
import sys
import argparse 
import random
from collections import defaultdict
from datetime import datetime

# Third party imports
import pandas as pd 
import numpy as np

# imports from local scripts
sys.path.append("./scripts")
import simple_path # last assigment script
from simple_path import StringDB, parse_input

# load in the the locus and build a function of 5000 random sub networks within the locus and def generate_random_network(n_networks: int) -> list:
def generate_random_subnetworks(fa_input, gene_counts):
	""" Generates random subnetworks where each gene is selected per locus_name

	Arguments:
	---------
	 fa_input : dict
		loaded FA loci data

	Results:
	-------
	list 
		Randomly selected genes
  
	limitation
	----------
	algortihm is O(N^2) when generating. Therefore increasing the number of generated
	random sublist will make it larger
	""" 
	networks = []
	for gene_list in fa_input.values():
		try:
			gene = gene_counts[random.choice(gene_list)]
		except KeyError:
			gene = 0
		networks.append(gene)
	return networks


def get_connected_genes(string_df):
	""" 
	Searches within STRING database and obtains informations genetic connectivity.
 
	Arguments:
	---------
	string_df : pd.DataFrame
		Pandas DataFrame of the STRING database

	Returns
	-------
	dict 
		Gene name and array of associated genes as key value pairs
	"""	
	# generating a set of all known genes within string database
	# -- both columns genes together and converts it into a set (removes all repeats)

	# grouping data based on gene name	
	grouped_df = string_df.groupby(by=["gene1"])
	connected_genes = defaultdict(None)
	for gene, gene_df in grouped_df:
		conn_genes_arr = gene_df["gene2"].values.tolist()
		connected_genes[gene] = conn_genes_arr

	return connected_genes


def get_gene_counts(connected_genes_data):
	"""
	Creates a sorted dictioanry of genes and its edge density.

	Arguments
	---------
	connected_genes_data : dict
		Dictioanry containing genes name as the key and all the other associated genes as the values

	Returns
	-------
	dict
		Gene name and edge desnity key value pairs,
	"""
	gene_counts = defaultdict(None)
	for gene, connected_genes in connected_genes_data.items():
		gene_counts[gene] = len(connected_genes)

	# sorts the genes by edge density
	sorted_gene_counts = {k: v for k, v in sorted(gene_counts.items(), key=lambda item: item[1])}

	return sorted_gene_counts	


def label_genes(counts):
    """ Provides genes with a unique number as and id
    
    Arguments
    ---------
	counts : dict
		Gene counts dictionary where each gene name has as asscoiated value, which is edge density
  
	Returns
	--------
	dict 
		labled genes with unique ids
    """
    labeled_genes = defaultdict(lambda: None)
    for idx, gene_name in enumerate(counts.keys()):
        labeled_genes[idx] = gene_name
    return labeled_genes


def bin_data(labeled_gene_counts, gene_ids, bins=128):
	"""Generates a binned dataframe where genes are placed into their
	appropriate bin based on edge density. 
 
	Arguments
	----------
	labeled_gene_counts : dict
		labeled data containing gene name as keys and edge density as values
	gene_ids : dict
		Unique id added to each gene
	bins : int, optional (default=128)
		Number of bins constructed within the binned dataframe

	Returns
	-------
	pd.DataFrame
		Binned genes based on edge density
  
	Limitations
	------------
	- Not all bins will have geenes in them because they do not meet the edge density criteria
	- Creates sub dataframes to handle different bins, increases memory usage
	"""
	gene_count_data = tuple(labeled_gene_counts.items())

	# bining data based edge density range using pandas.cut() function
	df = pd.DataFrame(data=gene_count_data, columns=["gene", "counts"])
	edges = np.linspace(df["counts"].values.min(), df["counts"].values.max(), bins+1).astype(int)
	labels = [f'({edges[i]}, {edges[i+1]})' for i in range(len(edges)-1)]
	z = pd.cut(df["counts"], include_lowest=True, bins=bins, labels=labels).to_frame(name="count_range")
	binned_data = z.groupby(by=["count_range"])


	# creating of for loop for adding gene name, edge_density and bin id
	dfs = []
	for bin_id, (name, bin_df) in enumerate(binned_data):
		end = bin_df.index.tolist()
		if len(end) == 0:
			# skip any bins that does not have any data
			continue 
		bin_df["gene_name"] = [gene_ids[idx] for idx in bin_df.index]
		bin_df["edge_density"] = [labeled_gene_counts[gene_ids[idx]] for idx in bin_df.index]
		bin_df["bin"] = bin_id + 1
		dfs.append(bin_df)
	
	binned_df = pd.concat(dfs)
	return binned_df


def permutation_test(ds1, ds2, n_shuffles=500):
    """ Motivations: Since selections is random and randomly selected genes is selected via bins, we can assume there is exchangeabiltiy
    
    
    Arguments:
    ----------
    ds1 : list, np.ndarray
		values of the first data set

	ds2 : list, np.ndarray
		values of the second dataset

	n_shuffles : int (default=500)
		number of shuffles conducting when runing the permutation test
	
	Returns
	-------
	float 
		Permutations test p-value 

    Limitations:
    ------------
    - Data must be exchangable, if not, the p-values will not be robust 
	- Increasing the number of shuffles will slow down the calculations. Has time complexity of O(N)
 
	Source
	------
 	https://stackoverflow.com/questions/24795535/pythons-implementation-of-permutation-test-with-permutation-number-as-input
    """
    n_samples, k = len(ds1), 0
    mean_diff = np.abs(np.mean(ds1) - np.mean(ds2))
    merged_ds = np.concatenate([ds1, ds2])
    
    for j in range(n_shuffles):
        np.random.shuffle(merged_ds)
        # conditional of the hpyothesis testing against the mean
        k += mean_diff <= np.abs(np.mean(merged_ds[:n_samples]) - np.mean(merged_ds[n_samples:]))
    return k / n_shuffles


def constuct_cofunctional_network(binned_data, density_array):
	""" Uses edge density value to find which bin the gene is from. Then selects a random gene and extracts its 
 	edge density
  
	binned_data : pd.DataFrame
		Contains genes that have been binned based on their edge densities
  
	density_array : list
		An array containing edge density values
  
	Returns
	-------
	list 
		Contains the edge density of the cofunctional network from genes selected from specific bins 
	"""

	cofunctional_network = []
	for edge_density in density_array:
		if edge_density != 0:
			# -- searches which bin the edge desnity value resides and returns the bin id
			bin_id = binned_data.loc[binned_data["edge_density"] == edge_density]["bin"].values[0]  

			# use the extracted bin id and extract all the edge_densities in the bin and select a random one
			random_edge_density = random.choice(binned_data.loc[binned_data["bin"] == bin_id]["edge_density"].values.tolist())
		else:
			random_edge_density = 0

		cofunctional_network.append(random_edge_density)

	return cofunctional_network

	
def sample_subnetworks(fa_data, gene_counts, binned_data, iterations=100, n_shuffles=100) -> float:
	""" Creates random subnetwork 
 
	Arguments
	---------
	fa_data : dict
		Dictioanry containing the Fanconi Anemia data input

	gene_counts : dict
		Dictioanry containing gene_name as keys and gene density as values
	
	binned_data : pd.DataFrame
		Contains genes that have been binned based on their edge densities

	iterations : int
		Number of randomy selected sub networks tested against randomly select networks 
		from the binned data. Each iteration genereated new subnetworks.

	n_shuffles : int
		Number of suffles conducted when applying the permutation test
 
	Results 
	------- 
	float 
		p-value of the sample
  
  
	Limitations
	-----------
	- Increasing the number of shuffles will require more computation time. O(N)
	- Increasing the number of iterations will require more computation time. O(N)
	- total time complexity is N^2
 	""" 

	if not isinstance(fa_data, dict):
		raise ValueError("Incorrect format provided, requires dictioanry, you have provided {}".format(type(fa_data)))

	cofunc_sum = []
	random_sum = []
	for iteration in range(iterations):
		random_sub_network = generate_random_subnetworks(fa_data, gene_counts)
		cofunctional_network = constuct_cofunctional_network(binned_data, random_sub_network)

		# checking cofunctional and random subnetworks are same size
		if len(random_sub_network) != len(cofunctional_network):
			raise RuntimeError("Both random and cofuctional networks are not the same size")

		# adding up edge desnity score and adding it 
		random_edge_sum = sum(random_sub_network)
		cofunc_edge_sum = sum(cofunctional_network)

		random_sum.append(random_edge_sum)
		cofunc_sum.append(cofunc_edge_sum)


	# permitations test 
	cofunc_arr = np.array(cofunc_sum)
	random_arr = np.array(random_sum)
	p = permutation_test(random_arr, cofunc_arr, n_shuffles=n_shuffles)
	return p


if __name__ == '__main__':

	# CLI Arguments will 
	parser = argparse.ArgumentParser(description="Simple program that creates random subnetworks and is compared to STRING database network") 
	required = parser.add_argument_group("Required Arguments")
	required.add_argument("-i", "--input", type=str, metavar="INPUT",
                        help="input file")
	parser.add_argument('-db', "--database", type=str, metavar="FILE",
                        help="path to database. Default path is `./Data/STRING.txt",
                        default="./Data/STRING.txt")
	parser.add_argument("-s", "--shuffles", type=int, metavar="PARAM",
                     	help="Number of shuffles done in permutations test. Default is 500",
                      	default=500)
	parser.add_argument("-x", "--iterations", type=int, metavar="PARAM",
                     	help="Number of sampling iterations. Default is 500",
                      	default=1000)
	parser.add_argument("--n_test", type=int, metavar="PARAM",
                     	help="Number of permutation test conducted",
                      	default=10)
	args = parser.parse_args()


	# loading in data and creating database object 
	fa_input = parse_input(args.input)
	db = StringDB(args.database)
	StringDB_df = db.to_pandas()

	# generating labeled data for genes
	# -- crearing edge density, connected_genes, unique_ids
	connected_genes = get_connected_genes(StringDB_df)
	gene_counts = get_gene_counts(connected_genes)
	id_gene = label_genes(gene_counts)

	# binning the data 
	binned_df = bin_data(gene_counts, id_gene)

	# sampling
	test_results = []
	for iteration in range(args.n_test):
		p_val  = sample_subnetworks(fa_input, gene_counts, binned_df, iterations=args.iterations, n_shuffles=args.shuffles)
		print("Permutation test score {}: {}".format(iteration+1, p_val))
		test_results.append([iteration, p_val])

	# converting into .csv file
	csv_outname = "pval_score_{}.csv".format(datetime.now().strftime("%m%d%y-%H%M%S"))
	results_df = pd.DataFrame(data=test_results, columns=["test", "p_val"])
	results_df.to_csv(csv_outname, index=False)
	